{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"euqcn01fKUsz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714811746476,"user_tz":-480,"elapsed":6471,"user":{"displayName":"Yunjia Liu","userId":"00294659283690096722"}},"outputId":"ecb3b2e1-0ef2-4fc4-d1ec-ecd8215688bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9534,"status":"ok","timestamp":1714811756533,"user":{"displayName":"Yunjia Liu","userId":"00294659283690096722"},"user_tz":-480},"id":"pQuF_gjNKpOj","outputId":"a43c4c18-cbdd-4c58-bc97-2307f41d7d48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.11.0)\n"]}],"source":["pip install torcheval"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5089,"status":"ok","timestamp":1714811761619,"user":{"displayName":"Yunjia Liu","userId":"00294659283690096722"},"user_tz":-480},"id":"HQGh-fdv-Ar-"},"outputs":[],"source":["import torch, json\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from functools import partial\n","torch.set_default_dtype(torch.float32)\n","class DataProgram:\n","  def __init__(self) -> None:\n","    self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n","    self.label2id = {\"O\":0, \"B\":1, \"M\":2, \"E\":3, \"S\":4}\n","    self.max_length = 512\n","\n","  def data_program(self, path, target_path):\n","    open(target_path, \"w\").write(\"\")\n","    init_data = open(path, \"r\")\n","    for line in init_data:\n","      cur_json = {\"text\": \"\", \"label\": []}\n","      # cur_json = {\"text\": \"\", \"label\": \"\"}\n","      line_list = line.strip(\"\\n\").split(\"  \")\n","      for t in line_list:\n","        cur_json[\"text\"] += t\n","        if len(t) == 1:\n","          cur_json[\"label\"].append(self.label2id[\"S\"])\n","          # cur_json[\"label\"] += \"S \"\n","        else:\n","          # cur_json[\"label\"] += \"B \"\n","          cur_json[\"label\"].append(self.label2id[\"B\"])\n","          for _ in range(len(t)-2):\n","            # cur_json[\"label\"] += \"M \"\n","            cur_json[\"label\"].append(self.label2id[\"M\"])\n","          # cur_json[\"label\"] += \"E \"\n","          cur_json[\"label\"].append(self.label2id[\"E\"])\n","      # cur_json[\"label\"] = cur_json[\"label\"][:-1]\n","      open(target_path, \"a\").write('{\\\"text\\\": \\\"'+ f'{cur_json[\"text\"]}'+'\\\", \\\"label\\\": \\\"'+ f'{cur_json[\"label\"]}'+'\\\"}\\n')\n","\n","  def convert_example_to_feature(self, example, is_infer=False):\n","    if not is_infer:\n","      example = example.map(lambda e: {\"text\": e[\"text\"], \"label\": json.loads(e[\"label\"])})\n","      # example = example.map(lambda e: {\"text\": e[\"text\"], \"label\": [self.label2id[item] for item in e[\"label\"].split(\" \")]})\n","      # assert len(example[\"label\"]) == len(example[\"text\"])\n","    dataset = example.map(lambda e: self.tokenizer(e['text'], truncation=True, padding='max_length'), batched=True)\n","    print(dataset)\n","    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n","    print(dataset)\n","    dataset = self.collate_fn(dataset)\n","    # print(dataset[\"label\"])\n","    # dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=self.collate_fn(dataset))\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n","    return dataloader\n","\n","  def collate_fn(self, batch_data, pad_id=0):\n","    input_ids_list, token_type_ids_list, label_list, attention_mask_list = [], [], [], []\n","    max_len = 0\n","    for example in batch_data:\n","      input_ids, token_type_ids, label, attention_mask = example[\"input_ids\"].tolist(), example[\"token_type_ids\"].tolist(), example[\"label\"].tolist(), example[\"attention_mask\"].tolist()\n","      input_ids_list.append(input_ids)\n","      token_type_ids_list.append(token_type_ids)\n","      label_list.append(label)\n","      attention_mask_list.append(attention_mask)\n","\n","      max_len = max(max_len, len(input_ids))\n","      max_len = max(max_len, len(token_type_ids))\n","      max_len = max(max_len, len(label))\n","      max_len = max(max_len, len(attention_mask))\n","    max_len = min(512, max_len)\n","    print(max_len)\n","    return batch_data.map(lambda e: {\"input_ids\": torch.Tensor(e[\"input_ids\"].tolist()[:max_len] + [pad_id] * max(0, (max_len - len(e[\"input_ids\"])))).float(),\n","                                        \"token_type_ids\": torch.Tensor(e[\"token_type_ids\"].tolist()[:max_len] + [pad_id] * max(0, (max_len - len(e[\"token_type_ids\"])))).float(),\n","                                        \"label\": torch.Tensor(e[\"label\"].tolist()[:max_len] + [pad_id] * max(0, (max_len - len(e[\"label\"])))).float(),\n","                                        \"attention_mask\": torch.Tensor(e[\"attention_mask\"].tolist()[:max_len] + [pad_id] * max(0, (max_len - len(e[\"attention_mask\"])))).float()\n","                                        })\n","    return batch_data.map(lambda e: {\"input_ids\": torch.Tensor(e[\"input_ids\"].tolist() + [pad_id] * (max_len - len(e[\"input_ids\"]))).float(),\n","                                           \"token_type_ids\": torch.Tensor(e[\"token_type_ids\"].tolist() + [pad_id] * (max_len - len(e[\"token_type_ids\"]))).float(),\n","                                           \"label\": torch.Tensor(e[\"label\"].tolist() + [pad_id] * (max_len - len(e[\"label\"]))).float(),\n","                                           \"attention_mask\": torch.Tensor(e[\"attention_mask\"].tolist() + [pad_id] * (max_len - len(e[\"attention_mask\"]))).float()\n","                                           })\n","\n","    for i in range(len(input_ids_list)):\n","      input_ids_list[i] = input_ids_list[i] + [pad_id] * (max_len - len(input_ids_list[i]))\n","      token_type_ids_list[i] = token_type_ids_list[i] + [pad_id] * (max_len - len(token_type_ids_list[i]))\n","      label_list[i] = label_list[i] + [pad_id] * (max_len - len(label_list[i]))\n","      attention_mask[i] = attention_mask[i] + [pad_id] * (max_len - len(attention_mask[i]))\n","    return torch.Tensor(input_ids_list), torch.Tensor(token_type_ids_list), torch.Tensor(label_list)\n","\n","  def get_dataloader(self):\n","    # self.data_program(\"/content/pku_train.utf8\", \"/content/train.json\")\n","    # self.data_program(\"/content/pku_dev.utf8\", \"/content/dev.json\")\n","    # self.data_program(\"/content/pku_test.utf8\", \"/content/test.json\")\n","    dataset = load_dataset(\"json\", data_files = {\"train\": \"/content/train.json\", \"test\":\"/content/test.json\", \"dev\": \"/content/dev.json\"})\n","    train_dataloader = self.convert_example_to_feature(dataset[\"train\"])\n","    dev_dataloader = self.convert_example_to_feature(dataset[\"dev\"])\n","    test_dataloader = self.convert_example_to_feature(dataset[\"test\"])\n","    # print(train_dataloader)\n","    # print(next(iter(train_dataloader)))\n","    return train_dataloader, dev_dataloader, test_dataloader"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"sba4GIFRJGmd","outputId":"4fbf8e96-66c0-43de-a83f-1d6cff55e4ec","executionInfo":{"status":"error","timestamp":1714812207382,"user_tz":-480,"elapsed":6194,"user":{"displayName":"Yunjia Liu","userId":"00294659283690096722"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'DataProgram' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-061bf8c80134>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataProgram' is not defined"]}],"source":["import torch.nn as nn\n","import torch, os, math\n","from transformers import AutoModelForMaskedLM\n","import numpy as np\n","import pandas as pd\n","torch.set_default_dtype(torch.float32)\n","train_dataloader, dev_dataloader, test_dataloader = DataProgram().get_dataloader()\n","model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n","num_epochs = 3\n","learning_rate = 3e-5\n","eval_steps = 100\n","log_steps = 10\n","save_dir = \"./checkpoints\"\n","\n","weight_decay = 0.01\n","warmup_proportion = 0.1\n","num_training_steps = len(train_dataloader) * num_epochs\n","\n","decay_params = [p.name for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"norm\"])]\n","\n","optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay = weight_decay)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metric_history = []\n","def evaluate(model, data_loader):\n","    accuracy = 0\n","    precision, recall, f1_score = [0 for i in range(5)], [0 for i in range(5)], [0 for i in range(5)]\n","    for batch_data in data_loader:\n","        batch_data[\"seq_len\"] = 0\n","        input_ids, token_type_ids, labels, attention_mask = batch_data[\"input_ids\"], batch_data[\"token_type_ids\"], batch_data[\"label\"], batch_data[\"attention_mask\"]\n","        logits = model(input_ids, token_type_ids).logits\n","        predictions = logits.argmax(axis=-1).float()\n","        mean = predictions.mean(dim=0).reshape(1, 512).repeat(predictions.size()[0], 1)\n","        std = predictions.std(dim=0, unbiased=False).reshape(1, 512).repeat(predictions.size()[0], 1)\n","        predictions = pow(2, - abs(predictions - mean) / std) / 2 * 10\n","        predictions = torch.tensor([[math.floor(i) for i in prediction] for prediction in predictions])\n","        accuracy += sum([sum([1 if labels[i][j] == predictions[i][j] else 0 for j in range(len(labels[i]))]) for i in range(len(labels))]) / (labels.size(0) * labels.size(1))\n","        for z in range(5):\n","            cur_precision = sum([sum([1 if (labels[i][j] == predictions[i][j] and predictions[i][j] == z) else 0 for j in range(len(labels[i]))]) for i in range(len(labels))]) / sum([sum([1 if (predictions[i][j] == z) else 0 for j in range(len(labels[i]))]) for i in range(len(labels))])\n","            cur_recall = sum([sum([1 if (labels[i][j] == predictions[i][j] and predictions[i][j] == z) else 0 for j in range(len(labels[i]))]) for i in range(len(labels))]) / sum([sum([1 if (labels[i][j] == z) else 0 for j in range(len(labels[i]))]) for i in range(len(labels))])\n","            cur_f1_score = 0 if (precision[z] + recall[z] == 0) else 2 * precision[z] * recall[z] / (precision[z] + recall[z])\n","            precision[z] += cur_precision\n","            recall[z] += cur_recall\n","            f1_score[z] += cur_f1_score\n","        print(precision, recall, f1_score)\n","        print(accuracy)\n","    precision, recall, f1_score = sum(precision) / 5, sum(recall) / 5, sum(f1_score) / 5\n","    return precision / len(data_loader), recall / len(data_loader), f1_score / len(data_loader)\n","\n","def train(model):\n","    model.train()\n","    global_step = 0\n","    best_score = 0.\n","    train_loss_record = []\n","    train_score_record = []\n","\n","    for epoch in range(num_epochs):\n","        for step, batch_data in enumerate(train_dataloader):\n","            inputs, token_type_ids, labels, attention_mask = batch_data[\"input_ids\"], batch_data[\"token_type_ids\"], batch_data[\"label\"], batch_data[\"attention_mask\"]\n","            outputs = model(input_ids=inputs, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","            sequence_output = outputs[0]\n","            print(sequence_output.size())\n","            print(labels.size())\n","            dropout = nn.Dropout(0.1)\n","            classifier = nn.Linear(21128, 512)\n","            logits = classifier(sequence_output)\n","            print(logits.size())\n","            print(labels.size())\n","            loss_fn = nn.CrossEntropyLoss()\n","            loss = loss_fn(logits, labels)\n","            print(loss)\n","            train_loss_record.append((global_step, loss.item()))\n","            loss.backward()\n","            optimizer.step()\n","\n","            if global_step % log_steps == 0:\n","                print(f\"[Train] epoch: {epoch} / {num_epochs}, step: {global_step} / {num_training_steps}, loss: {loss.item():.5f}\")\n","\n","            if global_step != 0 and (global_step % eval_steps == 0 or global_step == (num_training_steps - 1)):\n","                precision, recall, F1 = evaluate(model, dev_dataloader)\n","                train_score_record.append((global_step, F1))\n","\n","                model.train()\n","\n","                if F1 > best_score:\n","                    print(f\"[Evaluate] best accuracy performence has been updated: {best_score:.5f} --> {F1:.5f}\")\n","                    best_score = F1\n","\n","                    save_path = os.path.join(save_dir, \"best.pdparams\")\n","\n","                    torch.save(model.state_dict(), save_path)\n","                print(f\"[Evaluate] precision: {precision:.5f}, recall: {recall:.5f}, devscore: {F1:.5f}\")\n","            global_step += 1\n","    save_path = os.path.join(save_dir, \"final.pdparams\")\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"[Train] Training done!\")\n","\n","    return train_loss_record, train_score_record\n","train_loss_record, train_score_record = train(model)\n","# precision, recall, F1 = evaluate(model, dev_dataloader)"]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1714310766330}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}