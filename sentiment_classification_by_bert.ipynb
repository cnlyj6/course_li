{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmUziD1/EXJjlg+LNr4qBO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LsVNNOEMYdcC"},"outputs":[],"source":["# def tsv_to_json(path, target):\n","#     open(target, \"w\").write(\"\")\n","#     with open(path, \"r\", encoding=\"utf-8\") as fr:\n","#         for line in fr:\n","#             sentence_label = line.strip('\\n').strip().strip(\"\\t\").lower().split(\"\\t\")\n","#             sentence = ''.join(x for x in sentence_label[-1] if x.isalpha() or x == ' ')\n","#             label = 1 if int(\"\".join([c for c in sentence_label[0].split(\"_\")[1] if c.isdigit()])) >= 7 else 0\n","#             open(target, \"a\").write('{\\\"text\\\": \\\"'+ f'{sentence}'+'\\\", \\\"label\\\": '+ f'{label}'+'}\\n')\n","# tsv_to_json(\"./train.tsv\", \"./train.json\")\n","# tsv_to_json(\"./dev.tsv\", \"./dev.json\")\n","# tsv_to_json(\"./test.tsv\", \"./test.json\")\n","import numpy as np\n","from datasets import load_dataset\n","from transformers import BertTokenizer, BertModel, BertConfig\n","from functools import partial\n","import torch, os\n","from tools import plot_training_loss, plot_training_acc\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n","dataset = load_dataset(\"json\", data_files = {\"train\": \"train.json\", \"test\":\"test.json\", \"dev\": \"dev.json\"})\n","def convert_example_to_feature(examples, tokenizer, is_infer=False):\n","    encoded_inputs = tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n","    if not is_infer:\n","        encoded_inputs[\"labels\"] = [label for label in examples[\"label\"]]\n","    return encoded_inputs\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","trans_fn = partial(convert_example_to_feature, tokenizer=tokenizer)\n","train_dataset = dataset[\"train\"].map(trans_fn, batched=True)\n","dev_dataset = dataset[\"dev\"].map(trans_fn, batched=True)\n","test_dataset = dataset[\"test\"].map(trans_fn, batched=True)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n","dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=16)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n","# for key in next(iter(train_dataloader)).keys():\n","#     print((next(iter(train_dataloader))[key]))\n","#     break\n","\n","class BertForSequenceClassification(torch.nn.Module):\n","    def __init__(self, bert, num_classes=2, dropout=None):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.bert = BertModel.from_pretrained(bert)\n","        self.bert_config = BertConfig.from_pretrained(bert)\n","        self.dropout = torch.nn.Dropout(dropout if dropout is not None else self.bert_config.hidden_dropout_prob)\n","        self.classifier = torch.nn.Linear(self.bert_config.hidden_size, self.num_classes)\n","    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n","        outputs = self.bert(input_ids, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","model = BertForSequenceClassification('bert-base-uncased', 2)\n","decay_params = [p.name for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"norm\"])]\n","optimizer = torch.optim.Adam(params=model.parameters(), lr = 1e-5, weight_decay = 0.01)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","from tqdm import tqdm\n","def evaluate(model, data_loader):\n","    model.eval()\n","    for batch_data in tqdm(data_loader, desc=\"[Evaluation Progression]\"):\n","        input_ids, token_type_ids, labels, attention_mask = torch.stack(batch_data[\"input_ids\"]), torch.stack(batch_data[\"token_type_ids\"]), torch.Tensor(batch_data[\"labels\"]), torch.stack(batch_data[\"attention_mask\"])\n","        logits = model(input_ids=input_ids.t(), token_type_ids=token_type_ids.t(), attention_mask=attention_mask.t())\n","        logits = np.argmax(logits.detach(), axis=1)\n","        accuracy = accuracy_score(labels.detach(), logits.detach())\n","    return accuracy\n","\n","def train(model):\n","    model.train()\n","    num_epochs = 2\n","    eval_steps = 100\n","    global_step = 0\n","    best_score = 0.\n","    log_steps = 10\n","    save_dir = \"./checkpoints\"\n","    train_loss_record = []\n","    train_score_record = []\n","    num_training_steps = len(train_dataloader) * num_epochs\n","    for epoch in range(num_epochs):\n","        for step, batch_data in enumerate(train_dataloader):\n","            input_ids, token_type_ids, labels, attention_mask = torch.stack(batch_data[\"input_ids\"]), torch.stack(batch_data[\"token_type_ids\"]), torch.Tensor(batch_data[\"labels\"]), torch.stack(batch_data[\"attention_mask\"])\n","            logits = model(input_ids=input_ids.t(), token_type_ids=token_type_ids.t(), attention_mask=attention_mask.t())\n","            logits = np.argmax(logits.detach(), axis=1)\n","            loss = loss_fn(logits.float(), labels.float())\n","            train_loss_record.append((global_step, loss.item()))\n","            loss.requires_grad = True\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            if global_step % log_steps == 0:\n","                print(f\"[Train] epoch: {epoch} / {num_epochs}, step: {global_step} / {num_training_steps}, loss: {loss.item():.5f}\")\n","\n","            if global_step != 0 and (global_step % eval_steps == 0 or global_step == (num_training_steps - 1)):\n","                accuracy = evaluate(model, dev_dataloader)\n","                train_score_record.append((global_step, accuracy))\n","                print(f\"[Evaluate] dev score: {accuracy:.5f}\")\n","                model.train()\n","\n","                if accuracy > best_score:\n","                    print(f\"[Evaluate] best accuracy performence has been updated: {best_score:.5f} --> {accuracy:.5f}\")\n","                    best_score = accuracy\n","\n","                    save_path = os.path.join(save_dir, \"best.pdparams\")\n","\n","                    torch.save(model.state_dict(), save_path)\n","\n","            global_step += 1\n","    save_path = os.path.join(save_dir, \"final.pdparams\")\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"[Train] Training done!\")\n","\n","    return train_loss_record, train_score_record\n","train_loss_record, train_score_record = train(model)\n","\n","plot_training_loss(train_loss_record, \"./images/chapter7_bert_loss.pdf\", loss_legend_loc=\"upper right\", sample_step=60)\n","plot_training_acc(train_score_record, \"./images/chapter7_bert_acc.pdf\", acc_legend_loc=\"lower right\", sample_step=1)\n","\n","model.load_state_dict(torch.load(\"./checkpoints/best.pdparams\"))\n","accuracy = evaluate(model, test_dataloader)\n","print(f\"[Evaluate result] accuracy: {accuracy:.5f}\")\n","\n","def infer(model, text):\n","    model.eval()\n","    encoded_inputs = tokenizer(text, max_length = 512, truncation=True)\n","    input_ids = torch.Tensor(encoded_inputs[\"input_ids\"]).to(torch.int64).unsqueeze(0)\n","    token_type_ids = torch.Tensor(encoded_inputs[\"token_type_ids\"]).to(torch.int64).unsqueeze(0)\n","    logits = model(input_ids, token_type_ids)\n","    id2label = {0: \"消极情绪\", 1: \"积极情绪\"}\n","    max_label_id = np.argmax(logits.detach(), axis=1).numpy()[0]\n","    pred_label = id2label[max_label_id]\n","    print(\"Label: \", pred_label)\n","text = \"this movie is so good that I watch is several times.\"\n","infer(model, text)"]}]}